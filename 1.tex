%!TEX root = thesis.tex"`

\chapter{Задача мультиклассовой классификации }
\label{chapter-1}
Задача мультиклассовой классификации (\acrlong{mcc}) -- это задача о присвоении метки класса произвольному объекту в случае, когда количество возможных меток больше двух.

Более формально, пусть задано множество объектов $\mathbb{X}$, называемое \textit{пространством объектов}.
Требуется сопоставить объекту $x \in \mathbb{X}$ в соответствите \textbf{один} $y \in \mathbb{Y}$, $|\mathbb{Y}| = K$, называемый \textit{меткой класса}.
% Множество $\mathbb{Y}$ в общем случае называется \textit{целевой переменной}.
Отметим, что множество $\mathbb{X}$ может иметь произвольную природу: его элементами могут являться числа, вектора из $\mathbb{R}^n$, строки, представления изображений и т.д.
Аналогичным образом и $\mathbb{Y}$ может содержать элементы произвольной природы, характеризующие метку класса: это, к примеру, могут быть целые числа $0 \dots K$ или строки (например, <<кот>>, <<собака>>).

В практических приложениях чаще всего все данные приводят к численному виду: отображают либо в $\mathbb{R}^n$, либо в $\mathbb{N}$.
В частности, $K$ классов отображают во множество натуральных чисел $(0, \dots, K)$, категориальные признаки кодируют натуральными числами, строки преобразуют в вектора (в данной работе используется \acrshort{tf-idf}, о которой речь пойдет ниже) и т.д.

\section{Обзор способов обучения в зависимости от разметки}
В задаче классификации важную роль играет наличие преварительных примеров объектов с \textit{заранее заданной} меткой класса.
Наличие или отсутствие таких объектов кардинальным образом меняет подход и семейство алгоритмов, доступных для решения задачи.
С учетом этого задачи классификации можно поделить на следующие категории:
\begin{enumerate}
    \item \Gls{sup_learning} - существует известное заранее множество пар $(x_i, y_i)$, $i=1\dots l$, называемых \textit{обучающей выборкой}.
        Ставится задача предсказать метки $y_j$ для объектов, не вошедших в $\mathbb{X}_l = (x_1, \dots, x_l)$ (в обучающую выборку).
    \item \Gls{semisup_learning} - существует заранее известное множество пар $(x_i, y_i)$ и множество из $u$ неразмеченных объектов (объектов с неизвестной меткой) $x_{l+1}, \dots, x_{l+u} \in \mathbb{X}$.
        Ставится задача предсказать $y_j$ для объектов с неизвестной меткой класса.
    \item \Gls{unsup_learning} - не существует заранее известного множества размеченных объектов (в противоположность \gls{sup_learning}).
        Система должна обучиться без вмешательства извлекать закономерность в данных.
\end{enumerate}
В данной работе решается задача \textbf{\gls{sup_learning}}.
% TODO: (ruapyyj) рассказать, что есть бинарная классификация, и дать обзор ее методов <- Sun Apr  3 14:18:46 2022

\section{Бинарная классификация}
Многие методы мультиклассовой классификации берут начало от методов \textit{бинарной классификации} (\gls{binary_classification}).
Приведем их краткий обзор.
\subsection{Линейные классификаторы}
\Gls{linear_classifier} предсказывает метку класса на основе действия некоторой функции $f: \mathbb{R} \rightarrow \mathbb{R}$ на линейную комбинацию входных параметров объекта:
\begin{equation}
    \label{eq:linear_classifier}
    y = f (\vec{w} \cdot \vec{x}) = f \left( \sum_{j} w_j x_j \right)
\end{equation}

В качестве функции $f$ часто выступает \textit{пороговая функция}:

\begin{equation}
    \label{eq:threshold_func}
    f(\vec{x}) =
    \begin{cases}
        1, & \vec{w} \cdot \vec{x} > \theta \\
        0, & \text{иначе}
    \end{cases}
\end{equation}
при которой на выходе модели получаются конкретные метки.

Другой альтернативой служит использование функций $f: \mathbb{R} \rightarrow [0, 1]$, переводящих вектор в отрезок $[0, 1]$.
В таком случае на выходе модели получаются \textit{вероятности} $(p, 1 - p)$ отнесения объекта к классам 0 и 1.
Примером такой функции может служить \textit{сигмоида}:
\begin{equation}
    \label{eq:sigmoid}
    f (x) = \frac{1}{1 + e^{-x}}
\end{equation}
при которой вероятность отнесения к классу 0 объекта $\vec{x}$ будет иметь вид $p(0 | x_i, \vec{w}) = \cfrac{1}{1 + e^{-\vec{w} \cdot \vec{x}}}$
\subsection{Метод опорных векторов}
\Gls{svm} строит разделяющую гиперплоскость между двумя классами, также как и \gls{linear_classifier} при выборе пороговой функции.
Основное отличие состоит в том, что SVM строит плоскость так, чтобы сумма расстояний между плоскостью и точками одного класса была наибольшей и точки разных классов находились по разную сторону от плоскости.
В случае, когда данные невозможно разделить гиперплоскостью, используюся переменные мягкого отступа (\gls{soft_margin}) вместе с использованием ядер (\gls{kernel}).

Переменные мягкого отступа допускают <<попадание>> объектов другого класса в область разделения, в то время как ядро пытается преобразовать данные так, чтобы их стало возможным разделить гиперплоскостью. Коэффициенты перед \gls{soft_margin} и вид ядра являются гиперпараметрами модели (\gls{hyperparameters}).

\subsection{Деревья решений}
\label{par:decision-trees}
\Gls{decision-tree} - это класс нелинейных моделей, основанный на деревьях.
В каждом узле дерева делается некоторый \textit{прогноз}, на основе которого определяется следующая вершина или финальный ответ.
Чаще всего на практике используются \textit{бинарные решающие деревья} (\gls{binary-decision-tree}).

Более формально, пусть задано бинарное дерево, в котором:
\begin{enumerate}
    \item каждой внутренней вершине $v$ приписана функция (или предикат) $\beta (v) : \mathbb{X} \rightarrow \{0, 1\}$;
    \item каждой листовой вершине $v$ приписан прогноз $c_v~\in \mathbb{Y}$ (в случае с классификацией листу также может быть приписан вектор вероятностей).
\end{enumerate}
Рассмотрим теперь алгоритм $a(x)$, который стартует из корневой вершины $v_0$ и вычисляет значение функции $\beta (v_0)$.
Если оно равно нулю, то алгоритм переходит в левую дочернюю вершину, иначе в правую, вычисляет значение предиката в новой вершине, и делает переход или влево, или вправо.
Процесс продолжается, пока не будет достигнута листовая вершина; алгоритм возвращает тот класс, который приписан этой вершине.
Такой алгоритм называется \textit{бинарным решающим деревом}.

Из второго свойства следует важная особенность - решающие деревья способны решать задачу мультиклассовой классификации без сведения к бинарной классификации.
Для этого достаточно будет прогнозировать в листьях деревьев не скаляр, а вектор $\mathbb{R}^K$ -- это могут быть вероятности принадлежностей к каждому из классов, либо вектор из нулей и единиц (при этом в векторе будет не более одной единицы).

В данной работе в качестве алгоритма классификации используется как раз \gls{binary-decision-tree}.
\section{Обобщение бинарной классификации}
На момент написания работы существует значительное количество алгоритом для решения задачи \gls{binary_classification}.
Среди наиболее популярных выделяются линейные модели (логистическая регрессия \cite{logistic-regression-origins}, SVM \cite{svm_original}), непараметрические алгоритмы (KNN \cite{knn-original}) и деревья решений \cite{decision-trees-original}.
% TODO: (ruapyyj) добавить ссылки на источники, чтоб больше цитирований было <- Thu Apr 14 20:36:58 2022
При этом большая часть этих методов были разработаны для случая бинарной классификации, и их использование в мультиклассовой постановке требует некоторых модификаций.
Об этих модификациях и пойдет речь ниже.
\subsection{Сведение к бинарной}
Один из самых простых подходов, предполагающий разбиение исходной задачи на несколько задач бинарной классификации.
\subsubsection{Один-против-всех (\acrshort{ovr})}
В \acrshort{ovr} предлагается обучить $K$ бинарных классификаторов, каждый из которых относит объект либо к своему классу, либо к <<остальным>>.
Для $i$-ого классификатора метка $1$ означает принадлежность объекту классу $i$, а метка $0$ - принадлежность классу $j \neq i$.

Каждый классификатор $L_k$ на выходе оценку вероятности $f_k (x)$ объекта $\vec{x}$ к классу $k$, после чего предсказанным классом объявляется класс $\hat{y}~=~\underset{k \in K}{\mathrm{argmax}}\, f_k (\vec{x})$.

Тем не менее, такой подход имеет несколько недостатков (описаны в \cite{bishop}).
Во-первых, разные классификаторы обучены на разных данных, и нет никакой гарантии, что их предсказания $f_k (x)$ имеют соответствующие масштабы.
Во-вторых, при таком разбиении классы оказываются сильно сбалансированы.
К примеру, если изначальная выборка содержала 10 классов с одинаковым распределением меток, то при применении подхода \acrshort{ovr} каждый классификатор получит набор данных с 90\% меткой $0$ и 10\% меткой $1$.

Для второй проблемы существует модификация \cite{lee_N-1}, предлагающая модифицировать целевую переменную так, чтобы положительный класс имел метку $+1$, а отрицательный имел $\frac{-1}{K-1}$.
Это, тем не менее, не решает всех проблем данного подхода.
\subsubsection{Один-против-одного (\acrshort{ovo})}
Для решения проблем из подхода \acrshort{ovr} вполне естественно можно предложить сравнивать каждый класс не против <<остальных>>, а против некоторого другого класса, и обучать по классификатору на каждую пару классов.
Финальным ответом будет тот класс, за который <<проголосовало>> больше всего классификаторов.

Этот подход решает часть проблем из \acrshort{ovr}, но в то же время привносит неоднозначность для случае одинакового количества голосов в двух классах.
Кроме того, для из $K$ классов возможно составить $\frac{K (K - 1)}{2}$ пар, что приведет к обучению большего числа классификаторов, чем при \acrshort{ovr}.
\subsection{Расширение существующего алгоритма}
Некоторые алгоритмы классификации допускают обобщение на случай мультиклассовой классификации \cite{binclass-generalization-1}, \cite{binclass-generalization-2}, \cite{binclass-generalization-3}.
Популярный алгоритм \gls{decision-tree} допускают естественное обобщение на случай мультиклассовой классификации.
\subsubsection{Решающие деревья}
Как уже было сказано в \ref{par:decision-trees}, решающие деревья способны предсказывать вероятности отнесения объекта к произвольному классу.
Чтобы обобщить эту модель на случай нескольких классов, достаточно в листовых вершинах использовать функцию $\beta (v)$, возвращающую вектор $[0, 1]^K$ оценок вероятности принадлежности к каждому классу.
Итоговый класс по-прежнему можно определить через $\hat{y}~=~\underset{k \in K}{\mathrm{argmax}}\, f_k (\vec{x})$, где $f_k (\vec{x})$ - прогнозируемая моделью вероятность отнесения объекта $\vec{x} $ к классу $k$.

Данный подход позволит сразу использовать основанные на решающих деревьях алгоритмы с минимальной модификацией и в то же время получать все преимущества данного семейства (например, высокую обобщающую способность при объединении в \gls{random-forest} \cite{cite:random-forest}).
