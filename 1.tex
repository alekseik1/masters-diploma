%!TEX root = thesis.tex"`

\chapter{Задача мультиклассовой классификации }
Задача мультиклассовой классификации (\acrlong{mcc}) -- это задача о присвоении метки класса произвольному объекту в случае, когда количество возможных меток больше двух.

Более формально, пусть задано множество объектов $\mathbb{X}$, называемое \textit{пространством объектов}.
Требуется сопоставить объекту $x \in \mathbb{X}$ в соответствите \textbf{один} $y \in \mathbb{Y}$, $|\mathbb{Y}| = K$, называемый \textit{меткой класса}.
% Множество $\mathbb{Y}$ в общем случае называется \textit{целевой переменной}.
Отметим, что множество $\mathbb{X}$ может иметь произвольную природу: его элементами могут являться числа, вектора из $\mathbb{R}^n$, строки, представления изображений и т.д.
Аналогичным образом и $\mathbb{Y}$ может содержать элементы произвольной природы, характеризующие метку класса: это, к примеру, могут быть целые числа $0 \dots K$ или строки (например, <<кот>>, <<собака>>).

В практических приложениях чаще всего все данные приводят к численному виду: отображают либо в $\mathbb{R}^n$, либо в $\mathbb{N}$.
В частности, $K$ классов отображают во множество натуральных чисел $(0, \dots, K)$, категориальные признаки кодируют натуральными числами, строки преобразуют в вектора (в данной работе используется \acrshort{tf_idf}, о которой речь пойдет ниже) и т.д.

\section{Обзор способов обучения в зависимости от разметки}
В задаче классификации важную роль играет наличие преварительных примеров объектов с \textit{заранее заданной} меткой класса.
Наличие или отсутствие таких объектов кардинальным образом меняет подход и семейство алгоритмов, доступных для решения задачи.
С учетом этого задачи классификации можно поделить на следующие категории:
\begin{enumerate}
    \item \Gls{sup_learning} - существует известное заранее множество пар $(x_i, y_i)$, $i=1\dots l$, называемых \textit{обучающей выборкой}.
        Ставится задача предсказать метки $y_j$ для объектов, не вошедших в $\mathbb{X}_l = (x_1, \dots, x_l)$ (в обучающую выборку).
    \item \Gls{semisup_learning} - существует заранее известное множество пар $(x_i, y_i)$ и множество из $u$ неразмеченных объектов (объектов с неизвестной меткой) $x_{l+1}, \dots, x_{l+u} \in \mathbb{X}$.
        Ставится задача предсказать $y_j$ для объектов с неизвестной меткой класса.
    \item \Gls{unsup_learning} - не существует заранее известного множества размеченных объектов (в противоположность \gls{sup_learning}).
        Система должна обучиться без вмешательства извлекать закономерность в данных.
\end{enumerate}
В данной работе решается задача \textbf{\gls{sup_learning}}.
% TODO: (ruapyyj) рассказать, что есть бинарная классификация, и дать обзор ее методов <- Sun Apr  3 14:18:46 2022

\section{Бинарная классификация}
Многие методы мультиклассовой классификации берут начало от методов \textit{бинарной классификации} (\gls{binary_classification}).
Приведем их краткий обзор.
\subsection{Линейные классификаторы}
\Gls{linear_classifier} предсказывает метку класса на основе действия некоторой функции $f: \mathbb{R} \rightarrow \mathbb{R}$ на линейную комбинацию входных параметров объекта:
\begin{equation}
    \label{eq:linear_classifier}
    y = f (\vec{w} \cdot \vec{x}) = f \left( \sum_{j} w_j x_j \right)
\end{equation}

В качестве функции $f$ часто выступает \textit{пороговая функция}:

\begin{equation}
    \label{eq:threshold_func}
    f(\vec{x}) =
    \begin{cases}
        1, & \vec{w} \cdot \vec{x} > \theta \\
        0, & \text{иначе}
    \end{cases}
\end{equation}
при которой на выходе модели получаются конкретные метки.

Другой альтернативой служит использование функций $f: \mathbb{R} \rightarrow [0, 1]$, переводящих вектор в отрезок $[0, 1]$.
В таком случае на выходе модели получаются \textit{вероятности} $(p, 1 - p)$ отнесения объекта к классам 0 и 1.
Примером такой функции может служить \textit{сигмоида}:
\begin{equation}
    \label{eq:sigmoid}
    f (x) = \frac{1}{1 + e^{-x}}
\end{equation}
при которой вероятность отнесения к классу 0 объекта $\vec{x}$ будет иметь вид $p(0 | x_i, \vec{w}) = \cfrac{1}{1 + e^{-\vec{w} \cdot \vec{x}}}$
\subsection{Метод опорных векторов}
\Gls{svm} строит разделяющую гиперплоскость между двумя классами, также как и \gls{linear_classifier} при выборе пороговой функции.
Основное отличие состоит в том, что SVM строит плоскость так, чтобы сумма расстояний между плоскостью и точками одного класса была наибольшей и точки разных классов находились по разную сторону от плоскости.
В случае, когда данные невозможно разделить гиперплоскостью, используюся переменные мягкого отступа (\gls{soft_margin}) вместе с использованием ядер (\gls{kernel}).

Переменные мягкого отступа допускают <<попадание>> объектов другого класса в область разделения, в то время как ядро пытается преобразовать данные так, чтобы их стало возможным разделить гиперплоскостью. Коэффициенты перед \gls{soft_margin} и вид ядра являются гиперпараметрами модели (\gls{hyperparameters}).
\subsection{Деревья решений}

\section{Обобщение бинарной классификации}
\subsection{Сведение к бинарной}
\subsubsection{Один-против-всех (\acrshort{ovr})}
\subsubsection{Один-против-одного (\acrshort{ovo})}
\subsection{Расширение существующего алгоритма}
\subsubsection{Решающие деревья}
\section{Структура обучающих данных}
В настоящей работе в качестве входных данных выступают документы, выгруженные из открытых источников в сети Интернет.
Для решения этой задачи существует множество подходов, обладающие своими достоиствами и недостатками.
Остановимся кратко на наиболее популярных из них.
